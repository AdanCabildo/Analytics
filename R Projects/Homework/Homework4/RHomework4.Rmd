---
title: ANLYTC3 RB Homework R
author: Adan L. Cabildo BSCS-SS212
output: html_document
---

#### Instructions 

The due date is 7:30AM on Feb 14 (Wed). Submit BOTH the Knit HTML file and the Rmd file. If there are errors and it does not knit, submit the Rmd file of your answers for possible partial points.

Code should be clearly commented. Plots should be presentable and properly labeled/titled. Mitigate overplotting whenever possible. 

You may hard code whatever written answers are required.

#### Preliminaries

Here are some libraries that you may need.
```{r}
library(ggplot2)
library(plyr)
library(reshape2)
library(knitr)
library(binom) 
```

We will use the data files `county_data.csv` and `county_ggplot.rda`, which were part of the lecture materials. We'll assume they are in the same directory as this .Rmd file.

```{r}
county.data = read.csv(file = 'county_data.csv', header=TRUE)
load('county_ggplot.rda')
```


#### Questions 

**Problem 1:** 

Make a scatterplot of the per capita violent crime rates for each county as function of the population. Remember to mitigate overplotting. Does this plot resemble those for deaths, births, or infant deaths that we saw in the notes? If not, what is the biggest difference?

Note: you may want to use a log transform on the x-axis to see the data more clearly.

```{r fig.width=6, fig.height=4, dpi=80, fig.align='center'}
# fill in with the answers discussed in class
```

ANS: 

**Problem 2:**

Create a function called `find.std.residual()`, which lets you specify a column in the `county.data` data frame, and then uses the p-value approach to identify counties where people may potentially be at higher risk. 

Specifically, your function should take the following inputs

* the `county.data` data set
* `variable`: the name of the column that you want to inspect
* `null.prob`: this variable determines the null hypothesis that you will compare all counties to. Specifically, the null is that all counties are `binomial(population, null.prob)` random variables.

Your function should return a data frame with all of the columns in `county.data`, plus the following additional columns

* `expected.null`: the expected number of outcomes under the null hypothesis for each county
* `variance.null`: the variance under the null hypothesis for each county
* `residual`: the difference between the observed count and that predicted under the null
* `std.residual`: the standardized residual

The rows of the data frame should be sorted in decreasing order by `std.residual`

Be sure to comment your function thoroughly so that it is easy to understand later.

```{r fig.width=8, fig.height=6, dpi=100, fig.align='center'}
# fill in with the answers discussed in class EXCEPT the last 2 lines

```

**Problem 3:**

Use your function from problem 2 to create a table showing the top fifteen counties for `violent.crimes`, under a null hypothesis that each county has a crime per capita rate of 1%.

```{r fig.width=8, fig.height=6, dpi=100, fig.align='center'}
# fill in
```

**Problem 4:**

Create a function `map.std.residual()` which takes the output of your function `find.std.residual()`, and creates a choropleth plot.

The inputs to the function should be

* `county.gg`: the data frame needed to draw the map
* `county.data`: the output from `find.std.residual()`

The output should be a map that can be printed, to which you can customize later by adding calls like `scale_fill_gradient2()` and `labs()`

Use this function to create a choropleth plot of the results from problem 3.

```{r fig.width=10, fig.height=6, dpi=100, fig.align='center'}
# fill in
```

**Problem 5:**

Using the functions you wrote, make a new choropleth plot and table, this time using the US violent crime per capita rate as your null hypothesis. 

```{r fig.width=10, fig.height=6, dpi=100, fig.align='center'}
# fill in

```

Comment on why the map and table from problem 5 are different from those of problems 4 and 3.


